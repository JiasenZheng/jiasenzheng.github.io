<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Stereo Visual Odometry">
  <meta property="og:description" content="Built visual odometry with a stereo camera setup that estimates the position and motion of the vehicle.">

  <title>Stereo Visual Odometry</title>
  <!-- <title>Jiasen Zheng</title> -->
  <meta name="description" content="Built visual odometry with a stereo camera setup that estimates the position and motion of the vehicle.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>

<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>JIASEN ZHENG</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link " href="/contact/">Contact</a>

      <a class="nav-item nav-link " href="/resume/">Resume</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1 id="stereo-visual-odometry">Stereo Visual Odometry</h1>
<p><br /></p>
<h3 id="brief-overview">Brief overview</h3>
<p><br />
Visual odometry is the process of determining the position and orientation of a mobile robot by using camera images. A stereo camera setup and KITTI grayscale odometry dataset are used in this project. KITTI dataset is one of the most popular datasets and benchmarks for testing visual odometry algorithms. This project aims to use OpenCV functions and apply basic cv principles to process the stereo camera images and build visual odometry using the KITTI dataset.</p>

<h3 id="video-demo">Video demo</h3>
<div class="video">
  <iframe src="https://www.youtube.com/embed/k9cyyoGHMWg" frameborder="0" allowfullscreen=""></iframe>
</div>

<h3 id="algorithm-descriptions">Algorithm descriptions</h3>
<p><br /></p>
<h4 id="kitti-dataset">KITTI dataset:</h4>
<p><br />
KITTI is a real-world computer vision datasets that focus on various tasks, including stereo, optical flow, visual odometry, 3D object detection, and 3D tracking. In this project, only the visual odometry data will be used. For this task, only grayscale odometry data set and odometry ground-truth poses are needed. The grayscale image is used as the input image. The ground-truth poses are used to compare with the pose generated by the algorithm. Figure 1 shows the first grayscale frame of the left camera from the data set. More information about the sensor setup can be found <a href="http://www.cvlibs.net/datasets/kitti/setup.php">here</a>.</p>

<p><img src="http://localhost:4000/assets/kitti1.png" alt="alt text" /></p>
<center> Figure 1: The first grayscale frame of the left camera from the KITTI odometry data set </center>
<p><br /></p>

<h4 id="data-processing">Data processing:</h4>
<p><br />
The data is processed in the “Dataset_Handler” class. The class can store the image data and easily extract and use both image and sensor calibration data. Since the one folder of images will take around 30GB of RAM if stored as a list, a generator stores one frame of the image each time we call the function.</p>

<h4 id="calibration-data">Calibration data:</h4>
<p><br />
The calibration information can be found in the data set for each of the two cameras. The calibration data can be extracted in the form of the projection matrix. The projection matrix is a 3 by 4 matrix which contains information about a 3 by 3 intrinsic matrix of the camera, a 3 by 3 rotation matrix, and a 3 by 1 translation matrix of the camera pose.</p>

<h4 id="depth-calculation">Depth calculation:</h4>
<p><br />
In theory, it is difficult to obtain the depth information from a  single pinhole camera; however, it is possible using a stereo camera setup. In Figure 2, a schematic shows one target pixel projected in two image planes. We can solve for depth using similar triangles, as shown in Figure 3. In the equation, z is the depth of the pixel in meters, f is the focal length in the x-direction in pixels, b is the baseline distance between the two cameras which is 0.54m, and d is the disparity between the same points between the two images in pixels. We already know the camera’s focal length and baseline distance to calculate the depth. We still need to find the disparity between the left and right frame, described in the next section.
<img src="http://localhost:4000/assets/kitti2.png" alt="alt text" /></p>
<center> Figure 2: A schematic shows one target pixel projected in two image planes </center>
<p><br />
<img src="http://localhost:4000/assets/kitti3.png" alt="alt text" /></p>
<center> Figure 3: The solution of depth using similar triangles </center>
<p><br /></p>

<h4 id="disparity-calculation">Disparity calculation:</h4>
<p><br />
The disparity describes the difference between the same pixel projected to two separated cameras. There are a few ways of stereo matching available in OpenCV to find disparity. The two methods tested in the project are stereo block-matching (StereoBM) and stereo semi-global block-matching (StereoSGBM). Figure 4 below shows the disparity map using the stereo block-matching method, while Figure 5 shows the disparity map using the stereo semi-global block-matching.
<img src="http://localhost:4000/assets/kitti4.png" style="height: 200px; width:1000px;" /></p>
<center> Figure 4: The disparity map using the stereo block-matching method </center>
<p><br />
<img src="http://localhost:4000/assets/kitti5.png" style="height: 200px; width:1000px;" /></p>
<center> Figure 5: The disparity map using the stereo semi-global block-matching method </center>
<p><br /></p>

<p>The darker color indicates smaller disparity; however, the darkest color in the map (purple) indicates the regions that the matching method could not successfully detect. It is noticeable that there is one column on the left that the algorithm could not match, which makes sense because the two cameras are separated by a distance which will then cause a shift from the left camera to the right camera when recording images. Comparing the disparity maps rendered from the two methods, it is easy to tell that the semi-global block-matching method results in a smoother image with fewer dark gaps in the frame. However, the blocking-matching method only takes around 42 ms to process a frame, while the semi-global blocking-matching method takes 99 ms.</p>

<p>After calculating out the disparity map, we are ready to calculate the depth of the frame based on the depth formula provided in the previous section. Figure 6 shows the depth map after the calculation. The yellow pixels are the furthest regions, while the purple ones are the closest. The map is reasonable since the furthest region is the center of the image in the first frame, matching the original image.
<img src="http://localhost:4000/assets/kitti6.png" alt="alt text" /></p>
<center> Figure 6: The depth map of the first frame from the left camera with color bar </center>
<p><br /></p>

<h4 id="feature-extraction-and-match">Feature extraction and match:</h4>
<p><br />
In order to find and generate a path, we would need to find some key features and keep track of them. Those key features would be found between the current frame of the left camera and the next frame of the same camera. SIFT by David Lowe is a classic and robust feature extracting approach which computes the scale and rotation invariant feature descriptor. Another approach called ORB is also used as a comparison.</p>

<p>In this project, a Brute-Force matching algorithm is used to match all the features extracted previously. The algorithm takes the descriptor from the matching features of the current frame and is matched with all other features in the next frame using some distance calculation, and then chooses the closest one to match with.</p>

<p>Figures 7 and 8 show the two matching results using different feature extraction methods (SIFT and ORB). Moreover, they are both matched using Brute-Force. For SIFT algorithm, there are 2931 features found in total. It took around 100 ms for each of the two images to extract features and around 266 ms to match the features from the two frames. For the ORB algorithm, there are 500 features found in total. It took around 30 ms for each of the two images to extract features and around 70 ms to match the features from the two frames.
<img src="http://localhost:4000/assets/kitti7.png" style="height: 200px; width:1000px;" /></p>
<center> Figure 7: The matching map of two frames using SIFT feature extraction </center>
<p><br />
<img src="http://localhost:4000/assets/kitti8.png" style="height: 200px; width:1000px;" /></p>
<center> Figure 8: The matching map of two frames using ORB feature extraction </center>
<p><br />
SIFT approach has a better performance which detects five times more than the features extracted by ORB. However, the ORB approach is approximately three times faster than SIFT regarding the processing time. For both methods, especially the SIFT methods, we can find some matching lines that are not horizontal, which are wrong since the feature detected would not move too much in the y-direction from frame to frame. Then a method was proposed by David Lowe to filter out some outliers. A ratio test was implemented to determine the level of distinctiveness of features. The test compares the ratios of the distance of two matched descriptors and generates a probability density function for correct and incorrect matches based on the ratio of distances. Figure 9 shows the plot of the probability density function.
<img src="http://localhost:4000/assets/kitti9.png" alt="alt text" /></p>
<center> Figure 9: The probability density function for both correct and incorrect matches based on the ratio of distances (from David Lowe’s paper on SIFT)</center>
<p><br /> 
From the plot, we can conclude that the closer the two distances of the two matches are, the less likely they are correct matches. Also, the chance of correct matches hits a peak when at a ratio of distances of around 0.45. Therefore, a threshold of 0.45 was set in the project to filter out matches with ratios of distances higher than 0.45. After the low pass filter, the matches drop to 826 for SIFT approach and to 75 for the ORB approach. Figure 10 shows the matching map using SIFT method after the low pass filter. The matching lines are all nearly horizontal after the filter.
<img src="http://localhost:4000/assets/kitti10.png" style="height: 200px; width:1000px;" /></p>
<center> Figure 10: The matching map using SIFT method after the low pass filter with a threshold of 0.45 </center>
<p><br /></p>

<h4 id="motion-estimation">Motion estimation:</h4>
<p><br />
This section needs to solve for a rotation matrix and a translation vector for each frame. The PnP algorithm with RANSAC is an effective solver handling outliers well. The PnP solver takes a list of 1 by 3 vectors of 3D coordinates in meters from the current frame, a list of 1 by 2 vectors of 2D coordinates in pixels from the next frame, the intrinsic matrix of the camera, and then solve for the rotational matrix and translation vector of that frame. RANSAC minimizes the reprojection error and makes this function robust to outliers.</p>

<h4 id="visual-odometry-main-function">Visual odometry (main function):</h4>
<p><br />
In this section, the main function will be constructed, which calls the previously made functions to generate a trajectory based on the pose of each frame. For each iteration, a pair of subsequent frames are input to the PnP RANSAC solver, and then a pose consists of both rotation and translation matrix will be returned. A 4 by 4 homogenous transformation matrix will be constructed based on those two matrices. The transformation matrix calculated transforms the current frame to the next frame. We have to inverse the matrix and then multiply the previous transformation matrix to transform the next frame to the world frame. The world frame is the initial pose of the camera. The previous transformation matrix is an identity matrix for the first iteration and then accumulates for the rest iteration. Every pose of each iteration is then stored in a trajectory list and later used in plotting the estimated path.</p>

<h3 id="results">Results</h3>
<p><br />
Figure 11 shows the resultant plot of the estimated path and ground-truth path for scene 7 of the dataset. The ground Truth path is plotted as a blue line, and the estimated path is the yellow line. From the view of the left camera at the first frame, the positive z-direction is forward, positive y-direction is downward, and positive is rightward. After generating the trajectory of the estimated path, a calculation was implemented to calculate the distance between the endpoint of the ground-truth path and the estimated path. The distance is calculated to be 24.18 m for scene 7. Figure 12 shows the plot of both ground truth and estimated path using odometry data from scene 5. The distance between the endpoints of the ground truth and the estimated path is calculated to be 50.12 m.
<img src="http://localhost:4000/assets/kitti11.png" alt="alt text" /></p>
<center> Figure 11: The plot of ground truth path and estimated path obtained from visual odometry using scene 7 of KITTI odometry datasets </center>
<p><br />
<img src="http://localhost:4000/assets/kitti12.png" alt="alt text" /></p>
<center> Figure 12: The plot of ground truth path and estimated path obtained from visual odometry using scene 5 of KITTI odometry datasets </center>
<p><br /><br /></p>

<p class="text-center">
<a class="m-1 btn btn-outline-primary btn-2 " href="https://github.com/JiasenZheng/Stereo_Visual_Odometry">
  GitHub
</a>
</p>

</div>
  </main>

  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Jiasen Zheng</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:jiasenzheng2020@northwestern.edu"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/JiasenZheng"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/jiasen-zheng/"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a>

</div>

  <!---->
  <!-- <small id="attribution">
    theme <a href="https://github.com/YoussefRaafatNasry/portfolYOU">portfolYOU</a>
  </small> -->
  
</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>


</body>

</html>