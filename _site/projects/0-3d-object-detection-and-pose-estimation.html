<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="3D Object Detection and Pose Estimation">
  <meta property="og:description" content="Designed a computer vision setup and implemented software to detect objects and estimate their poses in 3D space.">

  <title>3D Object Detection and Pose Estimation</title>
  <!-- <title>Jiasen Zheng</title> -->
  <meta name="description" content="Designed a computer vision setup and implemented software to detect objects and estimate their poses in 3D space.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>

<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>JIASEN ZHENG</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link " href="/contact/">Contact</a>

      <a class="nav-item nav-link " href="/resume/">Resume</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1 id="point-cloud-object-detection-and-pose-estimation">Point Cloud Object Detection and Pose Estimation</h1>
<p><br /></p>
<h3 id="objective">Objective</h3>
<p><br />
<strong>Shape-Based Remote Manipulation (NSF)</strong>: We are building a system, consisting of an operating interface coupled with a manipulator, promises a communication across the distance barrier. 
<br />
<strong>My Focus</strong>: As the first stage of this project, I am responsible to develop a perception system to sense the manipulator’s environment.
<br />
<strong>Goal</strong>: This project aims to design a computer vision setup and implement software to detect objects and estimate their poses in 3D space.
<br /></p>
<h3 id="video-demo">Video demo</h3>
<div class="video">
  <iframe src="https://www.youtube.com/embed/uvhC8gluA9o" frameborder="0" allowfullscreen=""></iframe>
</div>
<p><br /></p>

<h3 id="hardware">Hardware</h3>
<ul>
  <li>Intel Realsense Lidar L515</li>
</ul>

<p>Intel realsense L515 uses a solid-state Lidar to sense depth. The Lidar can provide organized point clouds. It also has an RGB camera which is calibrated with the Lidar.</p>

<h3 id="pipeline">Pipeline</h3>
<p><br />
<img src="http://localhost:4000/assets/det_pipeline.png" />
<br /></p>

<h3 id="software">Software</h3>
<p>The software of this project can be explained in two stages based on the two ROS nodes I have in the project:</p>
<ul>
  <li>Inference server</li>
  <li>Perception core</li>
</ul>

<p><strong>Inference server</strong><br />
The inference server is a ROS node that runs a deep learning model (CNN) to detect objects in the image space. The inference server is implemented in Python using <a href="https://github.com/facebookresearch/detectron2">Detectron2</a> and Pytorch as the deep learning framework. For the use case of this project, a custom dataset is created to train the model. Mask-RCNN is used as the model architecture since we need to implement instance segmentation for our purposes. The model was then trained with 100 self-labeled images with an ontology of a “cheez-it” box. The labelled images are shown below:
<br />
<img src="http://localhost:4000/assets/annotation_results.gif" />
<br />
The images are annotated on an open-sourced plattform, <a href="https://github.com/opencv/cvat">CVAT</a>. CVAT provides many tools to accelerate the labeling processes and improve the qualities, such as machine learning models. 
<br />
The model was then trained using Detectron2 with 100 labeled images. The inference results are shown below:
<br />
<img src="http://localhost:4000/assets/instance_segmentation.gif" />
<br />
The inference server subscribes to the image topic and compute the target object masks. The inference server then publishes the masks to the perception core node.</p>

<p><strong>Perception core</strong><br />
The perception core is a ROS node that performs the following tasks:</p>
<ul>
  <li>Receives both the point cloud from the LiDAR and the image from the RGB camera</li>
  <li>Detects the table in point cloud data and remove it</li>
  <li>Denoises, downsamples, and clusters the point cloud data to create a foreground mask</li>
  <li>Receives the target object masks from the inference server and get a infrence mask</li>
  <li>merge the foreground mask and the inference mask to get a finak mask</li>
  <li>Performs oriented bounding box detection and pose estimation on the final mask with a state machine</li>
</ul>

<p>The perception core is implemented in C++ using ROS. The node uses PCL for point cloud process and OpenCV for image process. The organized point cloud acquired from the realsense Lidar helped to significantly reduce the runtime of the perception core. The algorithms in the node were implemented to keep the organized point cloud using a customized data structure, <strong>OrderedCloud</strong>. Moreover, some algorithm were implemented in image space to reduce the runtime such as <strong>denoise</strong> and <strong>cluster</strong>. As a result, the node can be run effeciently with real-time performance. A state machine is used to control the detection and pose estimation process based on the motion in the image space.</p>

<p>The below image shows the table removal process:
<br />
<img src="http://localhost:4000/assets/background_remover.gif" />
<br /></p>

<p>The below image shows the colorized point cloud clusters after denoising, downsampling, and clustering:
<br />
<img src="http://localhost:4000/assets/cluster.gif" />
<br /></p>

<p>The below image shows the CAD model replacement of the target objects:
<br />
<img src="http://localhost:4000/assets/3d_object_detection.gif" />
<br /></p>

<h3 id="future-scope">Future Scope</h3>
<ul>
  <li>Implement a faster CNN model for instance segmentation such as <a href="https://github.com/haotian-liu/yolact_edge">YOLACT-EDGE</a> to reduce the inference time</li>
  <li>Annotate more images to train a more robust and accurate model</li>
  <li>Annotate the model based on the cropped images to reduce the inference time</li>
  <li>Call the inference server with cropped images to reduce the inference time</li>
  <li>Implement a more robust and accurate pose estimation algorithm such as point cloud registration</li>
  <li>Multi-thread CPU implementation on the perception core node to reduce the runtime</li>
  <li>Implement some OpenCV and PCL algorithms in GPU to reduce the runtime</li>
</ul>

</div>
  </main>

  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Jiasen Zheng</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:jiasenzheng2020@northwestern.edu"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/JiasenZheng"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/jiasen-zheng/"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a>

</div>

  <!---->
  <!-- <small id="attribution">
    theme <a href="https://github.com/YoussefRaafatNasry/portfolYOU">portfolYOU</a>
  </small> -->
  
</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>


</body>

</html>